{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulate data (code from orignial source modified for windows and jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "from datetime import timedelta, datetime\n",
    "import numpy as np\n",
    "\n",
    "stress_data_path = \"C:/Users/jatin/OneDrive/Desktop/jstin/doi_10.5061_dryad.5hqbzkh6f__v6/Stress_dataset\"\n",
    "cpu_count = int(multiprocessing.cpu_count()/2)\n",
    "\n",
    "new_list = [\n",
    "    (file, sub_file) \n",
    "    for file in os.listdir(stress_data_path) \n",
    "    for sub_file in os.listdir(os.path.join(stress_data_path, file))\n",
    "]\n",
    "for i in range(len(new_list)):\n",
    "    file=new_list[i][0]\n",
    "    sub_file=new_list[i][1]\n",
    "    shutil.unpack_archive(\n",
    "        os.path.join(stress_data_path, file, sub_file), \n",
    "        os.path.join(stress_data_path, file, sub_file[:-4])\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 15\n",
      "Processing 5C\n",
      "Processing 6B\n",
      "Processing 6D\n",
      "Processing 7A\n",
      "Processing 7E\n",
      "Processing 83\n",
      "Processing 8B\n",
      "Processing 94\n",
      "Processing BG\n",
      "Processing CE\n",
      "Processing DF\n",
      "Processing E4\n",
      "Processing EG\n",
      "Processing F5\n",
      "Done Saving Data ...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "from datetime import timedelta, datetime\n",
    "import numpy as np\n",
    "\n",
    "DATA_PATH = \"C:/Users/jatin/OneDrive/Desktop/jstin/doi_10.5061_dryad.5hqbzkh6f__v6/Stress_dataset\"\n",
    "SAVE_PATH = \"C:/Users/jatin/OneDrive/Desktop/jstin/doi_10.5061_dryad.5hqbzkh6f__v6/processed_dataset\"\n",
    "os.mkdir(SAVE_PATH)\n",
    "\n",
    "final_columns = {\n",
    "    'ACC': ['id', 'X', 'Y', 'Z', 'datetime'],\n",
    "    'EDA': ['id', 'EDA', 'datetime'],\n",
    "    'HR': ['id', 'HR', 'datetime'],\n",
    "    'TEMP': ['id', 'TEMP', 'datetime'],\n",
    "}\n",
    "\n",
    "names = {\n",
    "    'ACC.csv': ['X', 'Y', 'Z'],\n",
    "    'EDA.csv': ['EDA'],\n",
    "    'HR.csv': ['HR'],\n",
    "    'TEMP.csv': ['TEMP'],\n",
    "}\n",
    "\n",
    "desired_signals = ['ACC.csv', 'EDA.csv', 'HR.csv', 'TEMP.csv']\n",
    "#desired_signals = ['ACC.csv', 'EDA.csv', 'HR.csv']\n",
    "\n",
    "acc = pd.DataFrame(columns=final_columns['ACC'])\n",
    "eda = pd.DataFrame(columns=final_columns['EDA'])\n",
    "hr = pd.DataFrame(columns=final_columns['HR'])\n",
    "temp = pd.DataFrame(columns=final_columns['TEMP'])\n",
    "\n",
    "def process_df(df, file):\n",
    "    start_timestamp = df.iloc[0,0]\n",
    "    sample_rate = df.iloc[1,0]\n",
    "    new_df = pd.DataFrame(df.iloc[2:].values, columns=df.columns)\n",
    "    new_df['id'] =  file[-2:]\n",
    "    new_df['datetime'] = [(start_timestamp + i/sample_rate) for i in range(len(new_df))]\n",
    "    return new_df\n",
    "c1=0\n",
    "c2=0\n",
    "c3=0\n",
    "c4=0\n",
    "for file in os.listdir(DATA_PATH):\n",
    "    print(f'Processing {file}')\n",
    "    for sub_file in os.listdir(os.path.join(DATA_PATH, file)):\n",
    "        if not sub_file.endswith(\".zip\"):\n",
    "            for signal in os.listdir(os.path.join(DATA_PATH, file, sub_file)):\n",
    "                if signal in desired_signals:\n",
    "                    df = pd.read_csv(os.path.join(DATA_PATH, file, sub_file, signal), names=names[signal], header=None)\n",
    "                    if not df.empty:\n",
    "                        if signal == 'ACC.csv':\n",
    "                            st=\"acc\"+str(c1)+\".csv\"\n",
    "                            process_df(df, file).to_csv(os.path.join(SAVE_PATH, st), index=False)\n",
    "                            c1+=1\n",
    "                        if signal == 'EDA.csv':\n",
    "                            st=\"eda\"+str(c2)+\".csv\"\n",
    "                            process_df(df, file).to_csv(os.path.join(SAVE_PATH, st), index=False)\n",
    "                            c2+=1\n",
    "                        if signal == 'HR.csv':\n",
    "                            st=\"hr\"+str(c3)+\".csv\"\n",
    "                            process_df(df, file).to_csv(os.path.join(SAVE_PATH, st), index=False)\n",
    "                            c3+=1\n",
    "                        if signal == 'TEMP.csv':\n",
    "                            st=\"temp\"+str(c4)+\".csv\"\n",
    "                            process_df(df, file).to_csv(os.path.join(SAVE_PATH, st), index=False)\n",
    "                            c4+=1\n",
    "\n",
    "print('Done Saving Data ...')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge all files now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "from datetime import timedelta, datetime\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143634730\n"
     ]
    }
   ],
   "source": [
    "SAVE_PATH = \"C:/Users/jatin/OneDrive/Desktop/jstin/doi_10.5061_dryad.5hqbzkh6f__v6/final\"\n",
    "c=0\n",
    "for i in range(609):\n",
    "    st=\"/\"+\"merged_data\"+str(i)+\".csv\"\n",
    "    df = pd.read_csv(SAVE_PATH+st, dtype={'id': str})\n",
    "    c+=df.shape[0]\n",
    "print(sorted(c,lambda x:x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/496 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 496/496 [22:43<00:00,  2.75s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "COMBINED_DATA_PATH = \"C:/Users/jatin/OneDrive/Desktop/jstin/doi_10.5061_dryad.5hqbzkh6f__v6/processed_dataset\"\n",
    "SAVE_PATH = \"C:/Users/jatin/OneDrive/Desktop/jstin/doi_10.5061_dryad.5hqbzkh6f__v6/final\"\n",
    "\n",
    "# if COMBINED_DATA_PATH != SAVE_PATH:\n",
    "#     os.mkdir(SAVE_PATH)\n",
    "\n",
    "print(\"Reading data ...\")\n",
    "for j in tqdm(range(113,609)):\n",
    "    \n",
    "    acc, eda, hr, temp = None, None, None, None\n",
    "\n",
    "    signals = ['acc', 'eda', 'hr', 'temp']\n",
    "    results=[]\n",
    "    for i in range(len(signals)):\n",
    "\n",
    "        signal=signals[i]\n",
    "        st=\"/\"+str(signal)+str(j)+\".csv\"\n",
    "        df = pd.read_csv(COMBINED_DATA_PATH+st, dtype={'id': str})\n",
    "        results.append([signal,df])\n",
    "    for i in results:\n",
    "        globals()[i[0]] = i[1]\n",
    "\n",
    "    # Merge data\n",
    "    ids = eda['id'].unique()\n",
    "    columns=['X', 'Y', 'Z', 'EDA', 'HR', 'TEMP', 'id', 'datetime']\n",
    "    results1=[]\n",
    "    for i in range(len(ids)):\n",
    "        id=ids[i]\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "\n",
    "        acc_id = acc[acc['id'] == id]\n",
    "        eda_id = eda[eda['id'] == id].drop(['id'], axis=1)\n",
    "        hr_id = hr[hr['id'] == id].drop(['id'], axis=1)\n",
    "        temp_id = temp[temp['id'] == id].drop(['id'], axis=1)\n",
    "\n",
    "        df = acc_id.merge(eda_id, on='datetime', how='outer')\n",
    "        df = df.merge(temp_id, on='datetime', how='outer')\n",
    "        df = df.merge(hr_id, on='datetime', how='outer')\n",
    "\n",
    "        df.fillna(method='ffill', inplace=True)\n",
    "        df.fillna(method='bfill', inplace=True)\n",
    "        results1.append(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    new_df = pd.concat(results1, ignore_index=True)\n",
    "\n",
    "    st=\"merged_data\"+str(j)+\".csv\"\n",
    "    new_df.to_csv(os.path.join(SAVE_PATH, st), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1 ...\n",
      "Reading 2 ...\n",
      "Converting ...\n",
      "Labelling ...\n",
      "5C is missing label 1.0 at 2020-04-15 13:00:00 to 2020-04-15 14:00:00\n",
      "5C is missing label 0.0 at 2020-06-12 07:00:00 to 2020-06-12 08:00:00\n",
      "6D is missing label 1.0 at 2020-06-03 07:00:00 to 2020-06-03 09:00:00\n",
      "7A is missing label 2.0 at 2020-07-07 19:16:00 to 2020-07-07 19:27:00\n",
      "7A is missing label 2.0 at 2020-07-07 19:50:00 to 2020-07-07 20:09:00\n",
      "7A is missing label 0.0 at 2020-07-07 20:24:00 to 2020-07-07 20:57:00\n",
      "Saving ...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Read Files\n",
    "print(\"Reading 1 ...\")\n",
    "PATH = 'C:/Users/t07/Downloads/jstin/doi_10.5061_dryad.5hqbzkh6f__v6/majid2'\n",
    "\n",
    "df = pd.read_csv(os.path.join(PATH, 'merged_data.csv'), dtype={'id': str})\n",
    "df['datetime'] = pd.to_datetime(df['datetime'].apply(lambda x: x * (10 ** 9)))\n",
    "\n",
    "print(\"Reading 2 ...\")\n",
    "survey_path = 'C:/Users/t07/Downloads/jstin/doi_10.5061_dryad.5hqbzkh6f__v6/SurveyResults.xlsx'\n",
    "\n",
    "survey_df = pd.read_excel(survey_path, usecols=['ID', 'Start time', 'End time', 'date', 'Stress level'], dtype={'ID': str})\n",
    "survey_df['Stress level'].replace('na', np.nan, inplace=True)\n",
    "survey_df.dropna(inplace=True)\n",
    "\n",
    "survey_df['Start datetime'] =  pd.to_datetime(survey_df['date'].map(str) + ' ' + survey_df['Start time'].map(str))\n",
    "survey_df['End datetime'] =  pd.to_datetime(survey_df['date'].map(str) + ' ' + survey_df['End time'].map(str))\n",
    "survey_df.drop(['Start time', 'End time', 'date'], axis=1, inplace=True)\n",
    "\n",
    "# Convert SurveyResults.xlsx to GMT-00:00\n",
    "print(\"Converting ...\")\n",
    "daylight = pd.to_datetime(datetime(2020, 11, 1, 0, 0))\n",
    "\n",
    "survey_df1 = survey_df[survey_df['End datetime'] <= daylight].copy()\n",
    "survey_df1['Start datetime'] = survey_df1['Start datetime'].apply(lambda x: x + timedelta(hours=5))\n",
    "survey_df1['End datetime'] = survey_df1['End datetime'].apply(lambda x: x + timedelta(hours=5))\n",
    "\n",
    "survey_df2 = survey_df.loc[survey_df['End datetime'] > daylight].copy()\n",
    "survey_df2['Start datetime'] = survey_df2['Start datetime'].apply(lambda x: x + timedelta(hours=6))\n",
    "survey_df2['End datetime'] = survey_df2['End datetime'].apply(lambda x: x + timedelta(hours=6))\n",
    "\n",
    "survey_df = pd.concat([survey_df1, survey_df2], ignore_index=True)\n",
    "# survey_df = survey_df.loc[survey_df['Stress level'] != 1.0]\n",
    "\n",
    "survey_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Label Data\n",
    "print('Labelling ...')\n",
    "ids = df['id'].unique()\n",
    "results=[]\n",
    "for i in range(len(ids)):\n",
    "    id=ids[i]\n",
    "    new_df = pd.DataFrame(columns=['X', 'Y', 'Z', 'EDA', 'HR', 'TEMP', 'id', 'datetime', 'label'])\n",
    "\n",
    "    sdf = df[df['id'] == id].copy()\n",
    "    survey_sdf = survey_df[survey_df['ID'] == id].copy()\n",
    "\n",
    "    for _, survey_row in survey_sdf.iterrows():\n",
    "        ssdf = sdf[(sdf['datetime'] >= survey_row['Start datetime']) & (sdf['datetime'] <= survey_row['End datetime'])].copy()\n",
    "\n",
    "        if not ssdf.empty:\n",
    "            ssdf['label'] = np.repeat(survey_row['Stress level'], len(ssdf.index))\n",
    "            new_df = pd.concat([new_df, ssdf], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"{survey_row['ID']} is missing label {survey_row['Stress level']} at {survey_row['Start datetime']} to {survey_row['End datetime']}\")\n",
    "        \n",
    "    results.append(new_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "new_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "print('Saving ...')\n",
    "new_df.to_csv(os.path.join(PATH, 'merged_data_labeled.csv'), index=False)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
